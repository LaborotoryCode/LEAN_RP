{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bc8cbb8-449a-4e4f-ae34-c7d899a98e3c",
   "metadata": {},
   "source": [
    "# <u>LLM App Development (Republic Polytechnic)</u> \n",
    "[07 Jan 2024]\n",
    "\n",
    "# <u>Day 2-05</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b517e27-050d-4032-aa5f-0c3282c9096d",
   "metadata": {},
   "source": [
    "# <font color=green>Setup and Installation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ae1e1-46b7-499a-9fd8-dd20f8524d9e",
   "metadata": {},
   "source": [
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and then pip install the libraries stated below.\n",
    "* If you want to run/experiement this Jupyter notebook in Google Colab, you have to enure that the relevant libraries are installed.\n",
    "* For best practice, an \"env.txt\" is created to store the OpenAI key.  This file should be in the same location as this Jupyter notebook.\n",
    "* If you are running/experimenting it at Google Colab, do remember to copy env.txt to your Google Colab drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd668d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29e471f-df41-43c7-92ee-f18dcbcc795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./RP-LLM/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: langchain in ./RP-LLM/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./RP-LLM/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./RP-LLM/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./RP-LLM/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./RP-LLM/lib/python3.11/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in ./RP-LLM/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in ./RP-LLM/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./RP-LLM/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (0.0.11)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (0.1.9)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (0.0.79)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./RP-LLM/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./RP-LLM/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./RP-LLM/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./RP-LLM/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./RP-LLM/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./RP-LLM/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./RP-LLM/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./RP-LLM/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./RP-LLM/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in ./RP-LLM/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in ./RP-LLM/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./RP-LLM/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./RP-LLM/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./RP-LLM/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./RP-LLM/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./RP-LLM/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./RP-LLM/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./RP-LLM/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./RP-LLM/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# once you have installed the libraries, you can comment it back\n",
    "# you only need to run this once\n",
    "!pip3 install openai langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87488f44-684a-4072-8e7d-9d413774eb20",
   "metadata": {},
   "source": [
    "## References\n",
    "- [LangChain Python Docs](https://python.langchain.com/en/latest/)\n",
    "- [LangChain Python API Reference](https://api.python.langchain.com/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60685f2-5621-48da-8c2a-febec503f3ef",
   "metadata": {},
   "source": [
    "## <font color=blue>Keep API Key Safe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46986fd7-351e-4a21-b2c2-8c71200d0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impport libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c0dcb3-ca50-4571-bc61-60c87893e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "# env.txt contains:\n",
    "# OPENAI_API_KEY=“sk-1234567890…………………..”\n",
    "\n",
    "openai_api_key = st.secrets['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04c3abe-b197-4d5a-8b9e-a544441f26fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolawinata/Documents/LEAN_RP/RP-LLM/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# use OpenAI as the LLM for evaluation\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model='gpt-3.5-turbo-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4578c6-ba51-4a2a-8e4d-70cbef3cfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cf73c-c109-416e-8062-6477c7a2bb59",
   "metadata": {},
   "source": [
    "## <font color=blue>Evaluations</font>\n",
    "\n",
    "In LangChain, the evaluation of Language Models (LLMs) involves multiple methods such as comparing chain outputs, pairwise string comparisons, string distances, and embedding distances. These evaluations help determine the most preferred model by analyzing the differences in their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94481fcd-6877-43fc-9774-0acf45ea63a9",
   "metadata": {},
   "source": [
    "### load_evaluator( )\n",
    "- Notice the parameter `llm` is optional and by default it is **None**.\n",
    "- Set the parameter `llm` to your desired LLM for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e527d319-b7b4-4246-8c31-e4cd483610bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5d4de2-a2d8-42a1-9146-0bdffb44ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_evaluator in module langchain.evaluation.loading:\n",
      "\n",
      "load_evaluator(evaluator: langchain.evaluation.schema.EvaluatorType, *, llm: Optional[langchain_core.language_models.base.BaseLanguageModel] = None, **kwargs: Any) -> Union[langchain.chains.base.Chain, langchain.evaluation.schema.StringEvaluator]\n",
      "    Load the requested evaluation chain specified by a string.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    evaluator : EvaluatorType\n",
      "        The type of evaluator to load.\n",
      "    llm : BaseLanguageModel, optional\n",
      "        The language model to use for evaluation, by default None\n",
      "    **kwargs : Any\n",
      "        Additional keyword arguments to pass to the evaluator.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Chain\n",
      "        The loaded evaluation chain.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from langchain.evaluation import load_evaluator, EvaluatorType\n",
      "    >>> evaluator = load_evaluator(EvaluatorType.QA)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd5e85-3329-48e0-8efc-474b08f36eeb",
   "metadata": {},
   "source": [
    "## Pairwise String Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eca0f62-1924-4bf6-aeb4-f8e16b6c78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"labeled_pairwise_string\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d02541-7057-4330-b4fa-b1da3b82b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_turbo='''\n",
    "LangChain is a Python library designed to make it easier to build applications with large language models, \n",
    "providing tools for chaining components and managing complex natural language processing workflows'''\n",
    "\n",
    "gpt35_0613_turbo='''\n",
    "LangChain is a Python framework that connects learners and tutors worldwide, \n",
    "offering personalized lessons, real-time practice, and transparent payment using blockchain technology \n",
    "to ensure security and fairness.'''\n",
    "\n",
    "# some criteria require reference labels to work correctly\n",
    "reference='''\n",
    "LangChain is a Python framework designed to streamline AI application development, focusing on real-time\n",
    "data processing and integration with Large Language Models.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=gpt4_turbo,\n",
    "    prediction_b=gpt35_0613_turbo,\n",
    "    input=\"describe LangChain in thirty words\",\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107f9b0a-e17b-4dae-b923-9a55a1d913f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82186419-2f6f-4a2c-894c-56c7abce7ed9",
   "metadata": {},
   "source": [
    "## Predefined Criteria - Conciseness\n",
    "`Cambridge Dictionary` The quality of being short and clear, and expressing what needs to be said without unnecessary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40766f7-1b9e-459e-b124-3c72a59c5803",
   "metadata": {},
   "source": [
    "### Concise Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41539680-e271-4604-8ca2-7011281b0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8dbc30-9da0-40c5-a3cd-32efb23ae6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise='''\n",
    "Generative AI is a type of artificial intelligence that can autonomously create new and original content, such as images, \n",
    "text, or other forms of data, using algorithms and models. It has the ability to generate outputs that mimic human-created \n",
    "content without relying solely on predefined patterns.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=concise,\n",
    "    input=\"What is generative AI?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1071165c-e9c9-4ca7-8f5a-8f6a9a48da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Determine if the submission is concise and to the point.\n",
      "- The submission provides a clear and brief definition of generative AI.\n",
      "- It includes relevant information about what generative AI is and how it works.\n",
      "- It does not include unnecessary details or go off-topic.\n",
      "- Therefore, the submission meets the criteria for conciseness.\n",
      "\n",
      "Conclusion: The submission meets the criteria for conciseness.\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9499aa-fc46-4917-a5be-c1a734ae8c56",
   "metadata": {},
   "source": [
    "### Inconcise Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afea760b-d278-4745-b986-6907e59920f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inconcise='''\n",
    "In the vast landscape of artificial intelligence, generative AI emerges as a subset intricately enmeshed in a labyrinthine \n",
    "array of algorithms, particularly those rooted in the complex neural networks exemplified by the captivating Generative \n",
    "Adversarial Networks (GANs). This convoluted field empowers machines to autonomously and creatively navigate the expansive \n",
    "spectrum of content creation, spanning from vividly evocative imagery to the nuanced articulation found in various textual expressions. \n",
    "This intricate process, laden with multifaceted intricacies, tangentially mirrors the profound subtleties inherent in the intricate \n",
    "tapestry of human cognition and expressive exploration.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=inconcise,\n",
    "    input=\"What is generative AI?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa43164d-e53b-45db-89fa-98b626c2e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Identify the criteria\n",
      "The criteria for this assessment is \"conciseness\", which evaluates whether the submission is concise and to the point.\n",
      "\n",
      "Step 2: Analyze the submission\n",
      "The submission is a lengthy and complex description of generative AI, with multiple layers of detail and intricate vocabulary.\n",
      "\n",
      "Step 3: Evaluate the submission based on the criteria\n",
      "The submission does not meet the criteria of conciseness as it is not brief and to the point. Instead, it is verbose and elaborate.\n",
      "\n",
      "Step 4: Determine the conclusion\n",
      "Based on the analysis, the submission does not meet the criteria of conciseness.\n",
      "\n",
      "Step 5: Print the result\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ec5ccb0-ef8c-4e32-99bc-6182b6da71f0",
   "metadata": {},
   "source": [
    "## Correctness\n",
    "`Cambridge Dictionary` The quality of being in agreement with the true facts or with what is generally accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e22a18-d447-4b95-9ad7-b93504803e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d10230-c982-40ea-ae7d-7c744b353b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopi C Kosong\n",
    "# Black coffee with evaporated milk and no sugar – think of it as a cafe au lait\n",
    "# ref: https://thehoneycombers.com/singapore/order-kopi-singapore/\n",
    "\n",
    "correctness_test_1='''\n",
    "    def solution(arr,queries):\n",
    "        for i in range(len(queries)):\n",
    "            ans = 0\n",
    "            for j in range(queries[i][0],queries[i][1]+1):\n",
    "                ans += arr[j]\n",
    "            print(ans)\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "reference='''\n",
    "    def solution(arr,queries):\n",
    "        prefix = [0] * len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            if i == 0:\n",
    "                prefix[i] = arr[i]\n",
    "            else:\n",
    "                prefix[i] = prefix[i - 1]+arr[i]\n",
    "        for q in queries:\n",
    "            print(prefix[q[1]-prefix[q[0]]+arr[q[0]]\n",
    "            \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffeef6a-5a7c-4876-bf41-6769600d50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: Step 1: Check for correctness\n",
      "- The submission appears to be correct as it takes in two parameters (arr and queries) and uses two for loops to iterate through the queries and calculate the sum of elements in the given range.\n",
      "- It also correctly prints out the sum for each pair in queries.\n",
      "\n",
      "Step 2: Check for accuracy\n",
      "- The submission accurately calculates the sum of elements in the given range, as it uses the correct index values and includes the last element in the range (queries[i][1]+1).\n",
      "\n",
      "Step 3: Check for factual\n",
      "- The submission follows the given task and input correctly, as it uses the correct parameters and follows the correct instructions for calculating the sum of elements in the given range.\n",
      "\n",
      "Conclusion: Based on the assessment, the submission meets all the criteria of correctness, accuracy, and factual. Therefore, the single character \"Y\" is printed on its own line, followed by a new line with just the letter \"Y\".\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "Based on the assessment, I would award the solution 3 marks.\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"\"\"write a function solution(arr,queries) which takes in 2 parameters, arr(first element is at index 0) and queries where arr is an integer list and queries is a 2-D list which contains pairs of integers. For every pair in queries, print the sum of elements in arr in the range pair[0] to pair[1]\n",
    "    \"\"\",\n",
    "    prediction=correctness_test_1,\n",
    "    reference=reference,\n",
    "\n",
    "\n",
    ")\n",
    "test = eval_result[\"reasoning\"]\n",
    "template = \"If the question is worth {marks} marks, based on your reasoning: {reason},how many marks would you award the solution?\"\n",
    "prompt = PromptTemplate(template=template,input_variables=[\"marks\",\"reason\"])\n",
    "chain = LLMChain(llm=llm,prompt=prompt,verbose=False)\n",
    "\n",
    "\n",
    "print(f'Reasoning: {eval_result[\"reasoning\"]}')\n",
    "print(chain.run(marks=3,reason=test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6137c-4d31-45ab-928f-d598cdc14087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0dac2e-b1d1-4d84-b4ea-abaff5e45ee9",
   "metadata": {},
   "source": [
    "## Custom Criteria\n",
    "- LangChain supports custom criteria and predefined principles for evaluation\n",
    "- Custom criteria can be defined using a key-value pairs {criterion_name : criterion_description}\n",
    "- These criteria can be used to assess outputs based on requirements or rubrics\n",
    "\n",
    "**Note**\n",
    "[LangChain](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain): it's recommended that you create a single evaluator per criterion. This way, separate feedback can be provided for each aspect. Additionally, if you provide antagonistic criteria, the evaluator won't be very useful, as it will be configured to predict compliance for ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "491f2b76-bdb4-4b49-9859-6718d0a357e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criteria = {\n",
    "    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n",
    "    \"clarity\": \"Are the sentences clear and easy to understand?\",\n",
    "    \"precision\": \"Is the writing precise, with no unnecessary words or details?\",\n",
    "    \"truthfulness\": \"Does the writing feel honest and sincere?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f92dcd-9dba-4d33-b42e-ca1b3d9253f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use default setting (None) for llm\n",
    "# this is because the API complained that:\n",
    "# This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n",
    "# Hence leave out setting the llm\n",
    "evaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4741f36-86b0-4d98-b2d9-630ccd9208fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=ChatPromptTemplate(input_variables=['input', 'prediction', 'prediction_b'], partial_variables={'reference': '', 'criteria': 'For this evaluation, you should primarily consider the following criteria:\\nsimplicity: Is the language straightforward and unpretentious?\\nclarity: Are the sentences clear and easy to understand?\\nprecision: Is the writing precise, with no unnecessary words or details?\\ntruthfulness: Does the writing feel honest and sincere?'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers \\the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['criteria', 'input', 'prediction', 'prediction_b'], template=\"{criteria}[User Question]\\n{input}\\n\\n[The Start of Assistant A's Answer]\\n{prediction}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{prediction_b}\\n[The End of Assistant B's Answer]\"))]) llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000013DCB67F760>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000013DCB6D56A0>, model_name='gpt-4', temperature=0.0, model_kwargs={'seed': 42}, openai_api_key='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', openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "# notice the model_name is gpt-4 when llm parameter is not set\n",
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b1c9e5-f5d5-4c05-9143-ff507f745d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody.\",\n",
    "    prediction_b=\"Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,\"\n",
    "                 \"identical notes; yet, every abode of despair conducts a dissonant orchestra, each \"\n",
    "                 \"playing an elegy of grief that is peculiar and profound to its own existence.\",\n",
    "    input=\"Write some prose about families.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe713c0-0d3b-4eca-b1a9-94acacf170bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"Assistant A's response is simpler, clearer, and more precise than Assistant B's. Assistant A uses straightforward language and clear sentences, making it easy to understand. On the other hand, Assistant B's response is more complex and uses more pretentious language, which makes it less clear and less straightforward. Both responses seem honest and sincere, so they are equal in terms of truthfulness. Therefore, considering the criteria of simplicity, clarity, precision, and truthfulness, Assistant A's response is superior. \\n\\nFinal Verdict: [[A]]\",\n",
       " 'value': 'A',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e439073-094f-4c91-aef5-9ba0515e2f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
